nohup: ignoring input
Seed set to 0
Reading camera 1/279Reading camera 2/279Reading camera 3/279Reading camera 4/279Reading camera 5/279Reading camera 6/279Reading camera 7/279Reading camera 8/279Reading camera 9/279Reading camera 10/279Reading camera 11/279Reading camera 12/279Reading camera 13/279Reading camera 14/279Reading camera 15/279Reading camera 16/279Reading camera 17/279Reading camera 18/279Reading camera 19/279Reading camera 20/279Reading camera 21/279Reading camera 22/279Reading camera 23/279Reading camera 24/279Reading camera 25/279Reading camera 26/279Reading camera 27/279Reading camera 28/279Reading camera 29/279Reading camera 30/279Reading camera 31/279Reading camera 32/279Reading camera 33/279Reading camera 34/279Reading camera 35/279Reading camera 36/279Reading camera 37/279Reading camera 38/279Reading camera 39/279Reading camera 40/279Reading camera 41/279Reading camera 42/279Reading camera 43/279Reading camera 44/279Reading camera 45/279Reading camera 46/279Reading camera 47/279Reading camera 48/279Reading camera 49/279Reading camera 50/279Reading camera 51/279Reading camera 52/279Reading camera 53/279Reading camera 54/279Reading camera 55/279Reading camera 56/279Reading camera 57/279Reading camera 58/279Reading camera 59/279Reading camera 60/279Reading camera 61/279Reading camera 62/279Reading camera 63/279Reading camera 64/279Reading camera 65/279Reading camera 66/279Reading camera 67/279Reading camera 68/279Reading camera 69/279Reading camera 70/279Reading camera 71/279Reading camera 72/279Reading camera 73/279Reading camera 74/279Reading camera 75/279Reading camera 76/279Reading camera 77/279Reading camera 78/279Reading camera 79/279Reading camera 80/279Reading camera 81/279Reading camera 82/279Reading camera 83/279Reading camera 84/279Reading camera 85/279Reading camera 86/279Reading camera 87/279Reading camera 88/279Reading camera 89/279Reading camera 90/279Reading camera 91/279Reading camera 92/279Reading camera 93/279Reading camera 94/279Reading camera 95/279Reading camera 96/279Reading camera 97/279Reading camera 98/279Reading camera 99/279Reading camera 100/279Reading camera 101/279Reading camera 102/279Reading camera 103/279Reading camera 104/279Reading camera 105/279Reading camera 106/279Reading camera 107/279Reading camera 108/279Reading camera 109/279Reading camera 110/279Reading camera 111/279Reading camera 112/279Reading camera 113/279Reading camera 114/279Reading camera 115/279Reading camera 116/279Reading camera 117/279Reading camera 118/279Reading camera 119/279Reading camera 120/279Reading camera 121/279Reading camera 122/279Reading camera 123/279Reading camera 124/279Reading camera 125/279Reading camera 126/279Reading camera 127/279Reading camera 128/279Reading camera 129/279Reading camera 130/279Reading camera 131/279Reading camera 132/279Reading camera 133/279Reading camera 134/279Reading camera 135/279Reading camera 136/279Reading camera 137/279Reading camera 138/279Reading camera 139/279Reading camera 140/279Reading camera 141/279Reading camera 142/279Reading camera 143/279Reading camera 144/279Reading camera 145/279Reading camera 146/279Reading camera 147/279Reading camera 148/279Reading camera 149/279Reading camera 150/279Reading camera 151/279Reading camera 152/279Reading camera 153/279Reading camera 154/279Reading camera 155/279Reading camera 156/279Reading camera 157/279Reading camera 158/279Reading camera 159/279Reading camera 160/279Reading camera 161/279Reading camera 162/279Reading camera 163/279Reading camera 164/279Reading camera 165/279Reading camera 166/279Reading camera 167/279Reading camera 168/279Reading camera 169/279Reading camera 170/279Reading camera 171/279Reading camera 172/279Reading camera 173/279Reading camera 174/279Reading camera 175/279Reading camera 176/279Reading camera 177/279Reading camera 178/279Reading camera 179/279Reading camera 180/279Reading camera 181/279Reading camera 182/279Reading camera 183/279Reading camera 184/279Reading camera 185/279Reading camera 186/279Reading camera 187/279Reading camera 188/279Reading camera 189/279Reading camera 190/279Reading camera 191/279Reading camera 192/279Reading camera 193/279Reading camera 194/279Reading camera 195/279Reading camera 196/279Reading camera 197/279Reading camera 198/279Reading camera 199/279Reading camera 200/279Reading camera 201/279Reading camera 202/279Reading camera 203/279Reading camera 204/279Reading camera 205/279Reading camera 206/279Reading camera 207/279Reading camera 208/279Reading camera 209/279Reading camera 210/279Reading camera 211/279Reading camera 212/279Reading camera 213/279Reading camera 214/279Reading camera 215/279Reading camera 216/279Reading camera 217/279Reading camera 218/279Reading camera 219/279Reading camera 220/279Reading camera 221/279Reading camera 222/279Reading camera 223/279Reading camera 224/279Reading camera 225/279Reading camera 226/279Reading camera 227/279Reading camera 228/279Reading camera 229/279Reading camera 230/279Reading camera 231/279Reading camera 232/279Reading camera 233/279Reading camera 234/279Reading camera 235/279Reading camera 236/279Reading camera 237/279Reading camera 238/279Reading camera 239/279Reading camera 240/279Reading camera 241/279Reading camera 242/279Reading camera 243/279Reading camera 244/279Reading camera 245/279Reading camera 246/279Reading camera 247/279Reading camera 248/279Reading camera 249/279Reading camera 250/279Reading camera 251/279Reading camera 252/279Reading camera 253/279Reading camera 254/279Reading camera 255/279Reading camera 256/279Reading camera 257/279Reading camera 258/279Reading camera 259/279Reading camera 260/279Reading camera 261/279Reading camera 262/279Reading camera 263/279Reading camera 264/279Reading camera 265/279Reading camera 266/279Reading camera 267/279Reading camera 268/279Reading camera 269/279Reading camera 270/279Reading camera 271/279Reading camera 272/279Reading camera 273/279Reading camera 274/279Reading camera 275/279Reading camera 276/279Reading camera 277/279Reading camera 278/279Reading camera 279/279
Reading camera 1/279Reading camera 2/279Reading camera 3/279Reading camera 4/279Reading camera 5/279Reading camera 6/279Reading camera 7/279Reading camera 8/279Reading camera 9/279Reading camera 10/279Reading camera 11/279Reading camera 12/279Reading camera 13/279Reading camera 14/279Reading camera 15/279Reading camera 16/279Reading camera 17/279Reading camera 18/279Reading camera 19/279Reading camera 20/279Reading camera 21/279Reading camera 22/279Reading camera 23/279Reading camera 24/279Reading camera 25/279Reading camera 26/279Reading camera 27/279Reading camera 28/279Reading camera 29/279Reading camera 30/279Reading camera 31/279Reading camera 32/279Reading camera 33/279Reading camera 34/279Reading camera 35/279Reading camera 36/279Reading camera 37/279Reading camera 38/279Reading camera 39/279Reading camera 40/279Reading camera 41/279Reading camera 42/279Reading camera 43/279Reading camera 44/279Reading camera 45/279Reading camera 46/279Reading camera 47/279Reading camera 48/279Reading camera 49/279Reading camera 50/279Reading camera 51/279Reading camera 52/279Reading camera 53/279Reading camera 54/279Reading camera 55/279Reading camera 56/279Reading camera 57/279Reading camera 58/279Reading camera 59/279Reading camera 60/279Reading camera 61/279Reading camera 62/279Reading camera 63/279Reading camera 64/279Reading camera 65/279Reading camera 66/279Reading camera 67/279Reading camera 68/279Reading camera 69/279Reading camera 70/279Reading camera 71/279Reading camera 72/279Reading camera 73/279Reading camera 74/279Reading camera 75/279Reading camera 76/279Reading camera 77/279Reading camera 78/279Reading camera 79/279Reading camera 80/279Reading camera 81/279Reading camera 82/279Reading camera 83/279Reading camera 84/279Reading camera 85/279Reading camera 86/279Reading camera 87/279Reading camera 88/279Reading camera 89/279Reading camera 90/279Reading camera 91/279Reading camera 92/279Reading camera 93/279Reading camera 94/279Reading camera 95/279Reading camera 96/279Reading camera 97/279Reading camera 98/279Reading camera 99/279Reading camera 100/279Reading camera 101/279Reading camera 102/279Reading camera 103/279Reading camera 104/279Reading camera 105/279Reading camera 106/279Reading camera 107/279Reading camera 108/279Reading camera 109/279Reading camera 110/279Reading camera 111/279Reading camera 112/279Reading camera 113/279Reading camera 114/279Reading camera 115/279Reading camera 116/279Reading camera 117/279Reading camera 118/279Reading camera 119/279Reading camera 120/279Reading camera 121/279Reading camera 122/279Reading camera 123/279Reading camera 124/279Reading camera 125/279Reading camera 126/279Reading camera 127/279Reading camera 128/279Reading camera 129/279Reading camera 130/279Reading camera 131/279Reading camera 132/279Reading camera 133/279Reading camera 134/279Reading camera 135/279Reading camera 136/279Reading camera 137/279Reading camera 138/279Reading camera 139/279Reading camera 140/279Reading camera 141/279Reading camera 142/279Reading camera 143/279Reading camera 144/279Reading camera 145/279Reading camera 146/279Reading camera 147/279Reading camera 148/279Reading camera 149/279Reading camera 150/279Reading camera 151/279Reading camera 152/279Reading camera 153/279Reading camera 154/279Reading camera 155/279Reading camera 156/279Reading camera 157/279Reading camera 158/279Reading camera 159/279Reading camera 160/279Reading camera 161/279Reading camera 162/279Reading camera 163/279Reading camera 164/279Reading camera 165/279Reading camera 166/279Reading camera 167/279Reading camera 168/279Reading camera 169/279Reading camera 170/279Reading camera 171/279Reading camera 172/279Reading camera 173/279Reading camera 174/279Reading camera 175/279Reading camera 176/279Reading camera 177/279Reading camera 178/279Reading camera 179/279Reading camera 180/279Reading camera 181/279Reading camera 182/279Reading camera 183/279Reading camera 184/279Reading camera 185/279Reading camera 186/279Reading camera 187/279Reading camera 188/279Reading camera 189/279Reading camera 190/279Reading camera 191/279Reading camera 192/279Reading camera 193/279Reading camera 194/279Reading camera 195/279Reading camera 196/279Reading camera 197/279Reading camera 198/279Reading camera 199/279Reading camera 200/279Reading camera 201/279Reading camera 202/279Reading camera 203/279Reading camera 204/279Reading camera 205/279Reading camera 206/279Reading camera 207/279Reading camera 208/279Reading camera 209/279Reading camera 210/279Reading camera 211/279Reading camera 212/279Reading camera 213/279Reading camera 214/279Reading camera 215/279Reading camera 216/279Reading camera 217/279Reading camera 218/279Reading camera 219/279Reading camera 220/279Reading camera 221/279Reading camera 222/279Reading camera 223/279Reading camera 224/279Reading camera 225/279Reading camera 226/279Reading camera 227/279Reading camera 228/279Reading camera 229/279Reading camera 230/279Reading camera 231/279Reading camera 232/279Reading camera 233/279Reading camera 234/279Reading camera 235/279Reading camera 236/279Reading camera 237/279Reading camera 238/279Reading camera 239/279Reading camera 240/279Reading camera 241/279Reading camera 242/279Reading camera 243/279Reading camera 244/279Reading camera 245/279Reading camera 246/279Reading camera 247/279Reading camera 248/279Reading camera 249/279Reading camera 250/279Reading camera 251/279Reading camera 252/279Reading camera 253/279Reading camera 254/279Reading camera 255/279Reading camera 256/279Reading camera 257/279Reading camera 258/279Reading camera 259/279Reading camera 260/279Reading camera 261/279Reading camera 262/279Reading camera 263/279Reading camera 264/279Reading camera 265/279Reading camera 266/279Reading camera 267/279Reading camera 268/279Reading camera 269/279Reading camera 270/279Reading camera 271/279Reading camera 272/279Reading camera 273/279Reading camera 274/279Reading camera 275/279Reading camera 276/279Reading camera 277/279Reading camera 278/279Reading camera 279/279/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[32m[INFO] Using 16bit Automatic Mixed Precision (AMP)[0m
[32m[INFO] GPU available: True (cuda), used: True[0m
[32m[INFO] TPU available: False, using: 0 TPU cores[0m
[32m[INFO] IPU available: False, using: 0 IPUs[0m
[32m[INFO] HPU available: False, using: 0 HPUs[0m
[32m[INFO] LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
[32m[INFO] 
  | Name            | Type                 | Params
---------------------------------------------------------
0 | perceptual_loss | PerceptualLoss       | 14.7 M
1 | text_segmentor  | LangSAMTextSegmentor | 0     
---------------------------------------------------------
0         Trainable params
14.7 M    Non-trainable params
14.7 M    Total params
58.865    Total estimated model params size (MB)[0m
[32m[INFO] Validation results will be saved to outputs/dge/turn_the_dozer_into_red@20240611-155757/save[0m

loaded pretrained LPIPS loss from threestudio/utils/lpips/vgg.pth
final text_encoder_type: bert-base-uncased
Model loaded from /data2/.cache/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth 
 => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])
  0%|          | 0/279 [00:00<?, ?it/s]  0%|          | 1/279 [00:00<02:54,  1.60it/s]  1%|▏         | 4/279 [00:00<00:42,  6.49it/s]  3%|▎         | 7/279 [00:00<00:26, 10.37it/s]  4%|▎         | 10/279 [00:01<00:20, 13.36it/s]  5%|▍         | 13/279 [00:01<00:16, 15.88it/s]  6%|▌         | 16/279 [00:01<00:14, 17.75it/s]  7%|▋         | 19/279 [00:01<00:13, 18.83it/s]  8%|▊         | 22/279 [00:01<00:12, 19.97it/s]  9%|▉         | 25/279 [00:01<00:12, 19.70it/s] 10%|█         | 28/279 [00:01<00:12, 20.40it/s] 11%|█         | 31/279 [00:02<00:11, 20.68it/s] 12%|█▏        | 34/279 [00:02<00:11, 21.09it/s] 13%|█▎        | 37/279 [00:02<00:11, 21.75it/s] 14%|█▍        | 40/279 [00:02<00:10, 22.03it/s] 15%|█▌        | 43/279 [00:02<00:10, 22.24it/s] 16%|█▋        | 46/279 [00:02<00:10, 22.38it/s] 18%|█▊        | 49/279 [00:02<00:10, 22.73it/s] 19%|█▊        | 52/279 [00:02<00:09, 23.11it/s] 20%|█▉        | 55/279 [00:03<00:09, 23.42it/s] 21%|██        | 58/279 [00:03<00:09, 23.70it/s] 22%|██▏       | 61/279 [00:03<00:09, 24.03it/s] 23%|██▎       | 64/279 [00:03<00:09, 23.64it/s] 24%|██▍       | 67/279 [00:03<00:08, 23.56it/s] 25%|██▌       | 70/279 [00:03<00:08, 23.75it/s] 26%|██▌       | 73/279 [00:03<00:08, 23.22it/s] 27%|██▋       | 76/279 [00:03<00:08, 23.08it/s] 28%|██▊       | 79/279 [00:04<00:08, 23.39it/s] 29%|██▉       | 82/279 [00:04<00:08, 23.63it/s] 30%|███       | 85/279 [00:04<00:08, 24.02it/s] 32%|███▏      | 88/279 [00:04<00:07, 24.38it/s] 33%|███▎      | 91/279 [00:04<00:07, 24.49it/s] 34%|███▎      | 94/279 [00:04<00:07, 24.40it/s] 35%|███▍      | 97/279 [00:04<00:07, 24.16it/s] 36%|███▌      | 100/279 [00:04<00:07, 24.35it/s] 37%|███▋      | 103/279 [00:05<00:07, 24.60it/s] 38%|███▊      | 106/279 [00:05<00:07, 24.42it/s] 39%|███▉      | 109/279 [00:05<00:07, 23.88it/s] 40%|████      | 112/279 [00:05<00:07, 23.75it/s] 41%|████      | 115/279 [00:05<00:06, 23.66it/s] 42%|████▏     | 118/279 [00:05<00:06, 23.59it/s] 43%|████▎     | 121/279 [00:05<00:06, 23.54it/s] 44%|████▍     | 124/279 [00:05<00:06, 23.30it/s] 46%|████▌     | 127/279 [00:06<00:06, 23.13it/s] 47%|████▋     | 130/279 [00:06<00:06, 22.78it/s] 48%|████▊     | 133/279 [00:06<00:06, 22.79it/s] 49%|████▊     | 136/279 [00:06<00:06, 23.17it/s] 50%|████▉     | 139/279 [00:06<00:05, 23.47it/s] 51%|█████     | 142/279 [00:06<00:05, 23.50it/s] 52%|█████▏    | 145/279 [00:06<00:05, 23.66it/s] 53%|█████▎    | 148/279 [00:06<00:05, 23.60it/s] 54%|█████▍    | 151/279 [00:07<00:05, 23.82it/s] 55%|█████▌    | 154/279 [00:07<00:05, 23.70it/s] 56%|█████▋    | 157/279 [00:07<00:05, 24.03it/s] 57%|█████▋    | 160/279 [00:07<00:04, 23.89it/s] 58%|█████▊    | 163/279 [00:07<00:04, 23.51it/s] 59%|█████▉    | 166/279 [00:07<00:04, 23.49it/s] 61%|██████    | 169/279 [00:07<00:04, 23.28it/s] 62%|██████▏   | 172/279 [00:07<00:04, 23.53it/s] 63%|██████▎   | 175/279 [00:08<00:04, 23.52it/s] 64%|██████▍   | 178/279 [00:08<00:04, 23.02it/s] 65%|██████▍   | 181/279 [00:08<00:04, 22.98it/s] 66%|██████▌   | 184/279 [00:08<00:04, 22.89it/s] 67%|██████▋   | 187/279 [00:08<00:03, 23.24it/s] 68%|██████▊   | 190/279 [00:08<00:03, 23.29it/s] 69%|██████▉   | 193/279 [00:08<00:03, 23.38it/s] 70%|███████   | 196/279 [00:09<00:03, 24.02it/s] 71%|███████▏  | 199/279 [00:09<00:03, 24.08it/s] 72%|███████▏  | 202/279 [00:09<00:03, 24.34it/s] 73%|███████▎  | 205/279 [00:09<00:03, 24.54it/s] 75%|███████▍  | 208/279 [00:09<00:02, 24.26it/s] 76%|███████▌  | 211/279 [00:09<00:02, 24.43it/s] 77%|███████▋  | 214/279 [00:09<00:02, 24.16it/s] 78%|███████▊  | 217/279 [00:09<00:02, 23.89it/s] 79%|███████▉  | 220/279 [00:10<00:02, 23.78it/s] 80%|███████▉  | 223/279 [00:10<00:02, 24.10it/s] 81%|████████  | 226/279 [00:10<00:02, 24.36it/s] 82%|████████▏ | 229/279 [00:10<00:02, 24.55it/s] 83%|████████▎ | 232/279 [00:10<00:01, 25.18it/s] 84%|████████▍ | 235/279 [00:10<00:01, 25.38it/s] 85%|████████▌ | 238/279 [00:10<00:01, 25.27it/s] 86%|████████▋ | 241/279 [00:10<00:01, 24.96it/s] 87%|████████▋ | 244/279 [00:10<00:01, 24.26it/s] 89%|████████▊ | 247/279 [00:11<00:01, 23.74it/s] 90%|████████▉ | 250/279 [00:11<00:01, 23.65it/s] 91%|█████████ | 253/279 [00:11<00:01, 23.16it/s] 92%|█████████▏| 256/279 [00:11<00:00, 23.29it/s] 93%|█████████▎| 259/279 [00:11<00:00, 23.72it/s] 94%|█████████▍| 262/279 [00:11<00:00, 24.09it/s] 95%|█████████▍| 265/279 [00:11<00:00, 24.12it/s] 96%|█████████▌| 268/279 [00:11<00:00, 23.93it/s] 97%|█████████▋| 271/279 [00:12<00:00, 23.99it/s] 98%|█████████▊| 274/279 [00:12<00:00, 24.28it/s] 99%|█████████▉| 277/279 [00:12<00:00, 24.26it/s]100%|██████████| 279/279 [00:12<00:00, 22.45it/s]
[32m[INFO] Segmentation with prompt: dozer[0m
Segment with prompt: dozer
  0%|          | 0/20 [00:00<?, ?it/s]/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/transformers/modeling_utils.py:862: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
  5%|▌         | 1/20 [00:02<00:44,  2.34s/it] 10%|█         | 2/20 [00:03<00:28,  1.56s/it] 15%|█▌        | 3/20 [00:04<00:22,  1.30s/it] 20%|██        | 4/20 [00:05<00:18,  1.18s/it] 25%|██▌       | 5/20 [00:06<00:16,  1.12s/it] 30%|███       | 6/20 [00:07<00:15,  1.08s/it] 35%|███▌      | 7/20 [00:08<00:13,  1.05s/it] 40%|████      | 8/20 [00:09<00:12,  1.03s/it] 45%|████▌     | 9/20 [00:10<00:11,  1.02s/it] 50%|█████     | 10/20 [00:11<00:10,  1.01s/it] 55%|█████▌    | 11/20 [00:12<00:09,  1.01s/it] 60%|██████    | 12/20 [00:13<00:08,  1.01s/it] 65%|██████▌   | 13/20 [00:14<00:07,  1.01s/it] 70%|███████   | 14/20 [00:15<00:06,  1.01s/it] 75%|███████▌  | 15/20 [00:16<00:05,  1.01s/it] 80%|████████  | 16/20 [00:17<00:04,  1.01s/it] 85%|████████▌ | 17/20 [00:18<00:03,  1.01s/it] 90%|█████████ | 18/20 [00:19<00:02,  1.00s/it] 95%|█████████▌| 19/20 [00:20<00:01,  1.01s/it]100%|██████████| 20/20 [00:21<00:00,  1.01s/it]100%|██████████| 20/20 [00:21<00:00,  1.07s/it]
[32m[INFO] Using prompt [turn the dozer into red] and negative prompt [][0m
[32m[INFO] Using view-dependent prompts [side]:[turn the dozer into red, side view] [front]:[turn the dozer into red, front view] [back]:[turn the dozer into red, back view] [overhead]:[turn the dozer into red, overhead view][0m
[32m[INFO] Loading InstructPix2Pix ...[0m
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  20%|██        | 1/5 [00:01<00:06,  1.57s/it]Loading pipeline components...:  60%|██████    | 3/5 [00:55<00:40, 20.25s/it]Loading pipeline components...: 100%|██████████| 5/5 [00:56<00:00, 10.19s/it]Loading pipeline components...: 100%|██████████| 5/5 [00:56<00:00, 11.30s/it]
[32m[INFO] Loaded InstructPix2Pix![0m
/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] /data2/manan/DGE/threestudio/systems/DGE.py:608: RuntimeWarning: invalid value encountered in arccos
  distances_with_sign = [np.arccos(np.dot(most_left_vecotr, cam.R[:, 2])) if np.dot(reference_axis,  np.cross(most_left_vecotr, cam.R[:, 2])) >= 0 else 2 * np.pi - np.arccos(np.dot(most_left_vecotr, cam.R[:, 2])) for cam in cams]
Start editing images...
Traceback (most recent call last):
  File "launch.py", line 253, in <module>
    main(args, extras)
  File "launch.py", line 196, in main
    trainer.fit(system, datamodule=dm, ckpt_path=cfg.resume)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self.fit_loop.run()
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/amp.py", line 80, in optimizer_step
    closure_result = closure()
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
    step_output = self._step_fn()
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 391, in training_step
    return self.lightning_module.training_step(*args, **kwargs)
  File "/data2/manan/DGE/threestudio/systems/DGE.py", line 634, in training_step
    self.edit_all_view(original_render_name='origin_render', cache_name="edited_views", update_camera=self.true_global_step >= self.cfg.camera_update_per_step, global_step=self.true_global_step) 
  File "/data2/manan/DGE/threestudio/systems/DGE.py", line 590, in edit_all_view
    edited_images = self.guidance(
  File "/data2/manan/DGE/threestudio/models/guidance/dge_guidance.py", line 425, in __call__
    edit_latents = self.edit_latents(text_embeddings, latents, cond_latents, t, cams)   # 20 x 4 x 64 x 64
  File "/data2/manan/DGE/threestudio/models/guidance/dge_guidance.py", line 227, in edit_latents
    self.forward_unet(latent_model_input, t, encoder_hidden_states=pivot_text_embeddings)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/data2/manan/DGE/threestudio/models/guidance/dge_guidance.py", line 138, in forward_unet
    return self.unet(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py", line 915, in forward
    sample, res_samples = downsample_block(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py", line 996, in forward
    hidden_states = attn(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/diffusers/models/transformer_2d.py", line 292, in forward
    hidden_states = block(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data2/manan/DGE/threestudio/utils/dge_utils.py", line 471, in forward
    self.attn_output = self.attn1(
  File "/data2/anaconda3/envs/DGE/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data2/manan/DGE/threestudio/utils/dge_utils.py", line 314, in forward
    sim_text = torch.bmm(q_text[:, j], k_text[:, j].transpose(-1, -2)) * self.scale
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.12 GiB (GPU 0; 23.64 GiB total capacity; 21.99 GiB already allocated; 135.44 MiB free; 23.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Epoch 0: |          | 0/? [00:03<?, ?it/s]